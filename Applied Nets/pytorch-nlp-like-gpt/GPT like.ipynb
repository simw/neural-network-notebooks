{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11966d5e-c0a4-480c-b526-27bd9a2903e1",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac1f8c-f988-4548-8ec1-8fd3308d1061",
   "metadata": {},
   "source": [
    "Related to / based on Karpathy videos / code at https://github.com/karpathy/nn-zero-to-hero and https://github.com/karpathy/nanoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965ebd50-f515-4c2c-9470-d5b62e799c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import time\n",
    "from collections.abc import Iterator\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import httpx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64315e0-664c-4e7e-bbd6-25824015babb",
   "metadata": {},
   "source": [
    "### Torch Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156fa07d-62f4-4c43-b284-45b7bbfaf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_torch_device() -> str:\n",
    "    return (\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97c876-6c45-4e55-80b7-08b3c094cc38",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c9390e-3623-4a39-a237-7a72bf0b76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_context_size: int,\n",
    "        input_dim: int,\n",
    "        head_size: int,\n",
    "        p_dropout: float,\n",
    "        num_heads: int,\n",
    "        p_multihead_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        output_dim = head_size * num_heads\n",
    "        self._key_query_value = torch.nn.Linear(input_dim, 3 * output_dim, bias=False)\n",
    "        self._dropout = torch.nn.Dropout(p_dropout)\n",
    "        self._projection = torch.nn.Linear(output_dim, output_dim)\n",
    "        self._multi_head_dropout = torch.nn.Dropout(p_multihead_dropout)\n",
    "\n",
    "        self._num_heads = num_heads\n",
    "        self._head_size = head_size\n",
    "        \n",
    "        tril = torch.tril(torch.ones(max_context_size, max_context_size))\n",
    "        self.register_buffer(\"tril\", tril)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        input_shape = x.shape\n",
    "        # x is (batch_size, context_size, input_dim)\n",
    "        # _key_query_value is (input_dim, 3 * head_size * num_heads)\n",
    "\n",
    "        kqv = self._key_query_value(x).view(x.shape[:-1] + (self._num_heads, self._head_size, 3))\n",
    "        # kqv: (batch_size, context_size, num_heads, head_size, 3)\n",
    "        keys = kqv[..., 0].transpose(1, 2)\n",
    "        queries = kqv[..., 1].transpose(1, 2)\n",
    "        values = kqv[..., 2].transpose(1, 2)\n",
    "        # keys, queries, values are (batch_size, num_heads, context_size, head_size)\n",
    "        \n",
    "        weights = queries @ keys.transpose(-2, -1) * self._head_size ** -0.5\n",
    "        # weights is (batch_size, num_heads, head_size, head_size)\n",
    "\n",
    "        context_size = x.shape[-2]\n",
    "        context_tril = self.tril[:context_size, :context_size]\n",
    "        weights = weights.masked_fill(context_tril == 0, float(\"-inf\"))\n",
    "        weights = torch.nn.functional.softmax(weights, dim=-1)\n",
    "        # weights is still (batch_size, num_heads, head_size, head_size)\n",
    "\n",
    "        weights = self._dropout(weights)\n",
    "        result = weights @ values\n",
    "        # result is (batch_size, context_size, head_size)\n",
    "\n",
    "        output_dim = self._num_heads * self._head_size\n",
    "        result = result.transpose(1, 2).contiguous().view(input_shape[:-1] + (output_dim, ))\n",
    "        result = self._projection(result)\n",
    "        result = self._multi_head_dropout(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "non_linearities = {\n",
    "    \"relu\": torch.nn.ReLU,\n",
    "    \"gelu\": torch.nn.GELU,\n",
    "}\n",
    "\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_hidden: int,\n",
    "        non_linearity: Literal[\"relu\", \"gelu\"],\n",
    "        n_output: int,\n",
    "        p_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_input, n_hidden),\n",
    "            non_linearities[non_linearity](),\n",
    "            torch.nn.Linear(n_hidden, n_output),\n",
    "            torch.nn.Dropout(p_dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_context_size: int,\n",
    "        input_dim: int,\n",
    "        p_attention_dropout: float,\n",
    "        num_attention_heads: int,\n",
    "        p_multihead_attention_dropout: float,\n",
    "        feed_forward_mult: int,\n",
    "        ff_nonlinearity: Literal[\"relu\", \"gelu\"],\n",
    "        p_ffwd_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if input_dim % num_attention_heads != 0:\n",
    "            msg = (\n",
    "                f\"Input dimension {input_dim} should be \"\n",
    "                f\"exactly divisible by {num_attention_heads}\"\n",
    "            )\n",
    "            raise Exception(msg)\n",
    "\n",
    "        head_size = input_dim // num_attention_heads\n",
    "        self._attention_layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        self._attention = MultiHeadAttention(\n",
    "            max_context_size,\n",
    "            input_dim,\n",
    "            head_size,\n",
    "            p_attention_dropout,\n",
    "            num_attention_heads,\n",
    "            p_multihead_attention_dropout,\n",
    "        )\n",
    "        self._ffwd_layer_norm = torch.nn.LayerNorm(input_dim)\n",
    "        attention_output_dim = head_size * num_attention_heads\n",
    "        self._feed_forward = FeedForward(\n",
    "            attention_output_dim,\n",
    "            attention_output_dim * feed_forward_mult,\n",
    "            ff_nonlinearity,\n",
    "            input_dim,\n",
    "            p_ffwd_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self._attention(self._attention_layer_norm(x))\n",
    "        x = x + self._feed_forward(self._ffwd_layer_norm(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_embeddings: int,\n",
    "        max_context_size: int,\n",
    "        num_attention_heads: int,\n",
    "        feed_forward_mult: int,\n",
    "        p_dropout: float,\n",
    "        ff_nonlinearity: str,\n",
    "        num_blocks: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._token_embedding_table = torch.nn.Embedding(vocab_size, n_embeddings)\n",
    "        self._position_embedding_table = torch.nn.Embedding(max_context_size, n_embeddings)\n",
    "        self._attention_blocks = torch.nn.Sequential(\n",
    "            *[\n",
    "                AttentionBlock(\n",
    "                    max_context_size,\n",
    "                    n_embeddings,\n",
    "                    p_dropout,\n",
    "                    num_attention_heads,\n",
    "                    p_dropout,\n",
    "                    feed_forward_mult,\n",
    "                    ff_nonlinearity,\n",
    "                    p_dropout,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ],\n",
    "        )\n",
    "        self._final_layer_norm = torch.nn.LayerNorm(n_embeddings)\n",
    "        self._final_linear = torch.nn.Linear(n_embeddings, vocab_size)\n",
    "        self._max_context_size = max_context_size\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor | None = None):\n",
    "        inputs = inputs[:, -self._max_context_size:]\n",
    "        batch_size, block_size = inputs.shape\n",
    "        # inputs: (batch_size, block_size)\n",
    "        # targets: (batch_size, block_size)\n",
    "\n",
    "        token_embeddings = self._token_embedding_table(inputs)\n",
    "        position_embeddings = self._position_embedding_table(torch.arange(block_size, device=inputs.device))\n",
    "        x = token_embeddings + position_embeddings\n",
    "        # x is (batch_size, block_size, n_embeddings)\n",
    "        x = self._attention_blocks(x)\n",
    "        logits = self._final_linear(self._final_layer_norm(x))\n",
    "        # # logits is (batch_size, block_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def _test_model() -> None:\n",
    "    torch_device = get_torch_device()\n",
    "\n",
    "    vocab_size = 11\n",
    "    n_embeddings = 9\n",
    "    max_context_size = 7\n",
    "    num_attention_heads = 3\n",
    "    feed_forward_mult = 4\n",
    "    p_dropout = 0.15\n",
    "    num_blocks = 3\n",
    "    \n",
    "    model = Model(\n",
    "        vocab_size,\n",
    "        n_embeddings,\n",
    "        max_context_size,\n",
    "        num_attention_heads,\n",
    "        feed_forward_mult,\n",
    "        p_dropout,\n",
    "        \"relu\",\n",
    "        num_blocks,\n",
    "    ).to(torch_device)\n",
    "\n",
    "    batch_size = 32\n",
    "    context_size = 4\n",
    "    x = torch.ones((batch_size, context_size), dtype=torch.long, device=torch_device)\n",
    "    logits = model(x)\n",
    "    assert logits.shape == (batch_size, context_size, vocab_size)\n",
    "\n",
    "\n",
    "_test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09eef62-3814-47b2-81ff-f3849de2df0d",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94781d25-185f-40eb-89cc-ba03c68916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(model, input: str, encoder, max_new_tokens: int, n_samples: int) -> list[str]:\n",
    "    torch_device = next(model.parameters()).device\n",
    "    tokens = [encoder.encode(input)] * n_samples\n",
    "    values = torch.tensor(tokens).to(torch_device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(values)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_value = torch.multinomial(probs, num_samples=1)\n",
    "        values = torch.cat((values, next_value), dim=1)\n",
    "\n",
    "    return [encoder.decode(sample.tolist()) for sample in values]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, loss_fn, dataloader, n_batches):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(n_batches)\n",
    "    torch_device = next(model.parameters()).device\n",
    "    for k in range(n_batches):\n",
    "        inputs, targets = next(dataloader)\n",
    "        logits = model(inputs.to(torch_device))\n",
    "        loss = loss_fn(logits, targets.to(torch_device))\n",
    "        losses[k] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    return losses.mean().item(), losses.std().item() / np.sqrt(len(losses))\n",
    "\n",
    "\n",
    "def count_params(model, depth: int = 0):\n",
    "    params = {name: param.numel() for name, param in model.named_parameters()}\n",
    "\n",
    "    result = {}\n",
    "    for name, param in params.items():\n",
    "        adjusted_name = \".\".join(name.split(\".\")[:depth])\n",
    "        result[adjusted_name] = result.get(adjusted_name, 0) + param\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be390831-9dd9-4f32-89a7-4a405e8d3c6b",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9310169-57b6-4152-bffc-d9eccca6f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:34:33,973 - INFO: Loading https://pytorch.org/ from remote\n",
      "2025-01-23 14:34:34,168 - INFO: HTTP Request: GET https://pytorch.org/ \"HTTP/1.1 200 OK\"\n",
      "2025-01-23 14:34:34,170 - INFO: Loading https://pytorch.org/ from cache at ./data/testing.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pytorch.org/ contents length 52835\n",
      "Cached contents length 52835\n"
     ]
    }
   ],
   "source": [
    "def get_data(url: str, cache_path: str) -> str:\n",
    "    file = Path(cache_path)\n",
    "    if not file.exists():\n",
    "        logger.info(f\"Loading {url} from remote\")\n",
    "        file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(file, \"w\") as file:\n",
    "            res = httpx.get(url)\n",
    "            file.write(res.text)\n",
    "        return res.text\n",
    "\n",
    "    logger.info(f\"Loading {url} from cache at {cache_path}\")\n",
    "    with open(cache_path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def _test_get_data() -> None:\n",
    "    test_path = \"./data/testing.txt\"\n",
    "    url = \"https://pytorch.org/\"\n",
    "\n",
    "    Path(test_path).unlink(missing_ok=True)\n",
    "\n",
    "    res = get_data(url, test_path)\n",
    "    print(f\"{url} contents length {len(res)}\")\n",
    "    assert len(res) > 0\n",
    "    assert Path(test_path).exists()\n",
    "    res2 = get_data(url, test_path)\n",
    "    print(f\"Cached contents length {len(res2)}\")\n",
    "\n",
    "    Path(test_path).unlink()\n",
    "\n",
    "\n",
    "_test_get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff17e8f7-391e-4a8a-bc7d-33fdf391e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochDataLoader():\n",
    "    def __init__(self, tokens: list[int], context_size: int, batch_size: int) -> None:\n",
    "        self._context_size = context_size\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        full_length = len(tokens)\n",
    "        tokens_per_batch = context_size * batch_size\n",
    "        num_batches = (full_length - 1) // tokens_per_batch\n",
    "        batched_length = tokens_per_batch * num_batches\n",
    "        self._used_tokens = torch.tensor(tokens[:(batched_length + 1)])\n",
    "        \n",
    "        starting_indices = context_size * torch.randperm(num_batches * batch_size).view(-1, batch_size, 1)\n",
    "        self._indices = starting_indices + torch.arange(context_size).view(1, 1, context_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._indices)\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[torch.Tensor, torch.Tensor]]:\n",
    "        for ixs in self._indices:\n",
    "            inputs = (\n",
    "                torch.gather(self._used_tokens, 0, ixs.view(-1))\n",
    "                .view(self._batch_size, self._context_size)\n",
    "            )\n",
    "            targets = (\n",
    "                torch.gather(self._used_tokens, 0, ixs.view(-1) + 1)\n",
    "                .view(self._batch_size, self._context_size)\n",
    "            )\n",
    "            yield inputs, targets\n",
    "\n",
    "\n",
    "class RandomDataLoader:\n",
    "    def __init__(self, tokens: list[int], context_size: int, batch_size: int) -> None:\n",
    "        self._data = torch.tensor(tokens)\n",
    "        self._context_size = context_size\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._data) // (self._context_size * self._batch_size)\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[torch.Tensor, torch.Tensor]]:\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # returns tuple of 2D tensors, (batch_size, block_size)\n",
    "        starting_indices = torch.randint(len(self._data) - self._context_size, (self._batch_size, ))\n",
    "        indices = torch.arange(self._context_size).unsqueeze(0) + starting_indices.unsqueeze(1)\n",
    "        inputs = torch.gather(self._data, 0, indices.view(-1)).view(self._batch_size, self._context_size)\n",
    "        targets = torch.gather(self._data, 0, indices.view(-1) + 1).view(self._batch_size, self._context_size)\n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def _test_epoch_dataloader() -> None:\n",
    "    data = list(range(100))\n",
    "    context_size = 3\n",
    "    batch_size = 8\n",
    "    dataloader = EpochDataLoader(data, context_size, batch_size)\n",
    "    assert len(dataloader) == 4\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    for inputs, targets in dataloader:\n",
    "        assert inputs.shape == (batch_size, context_size)\n",
    "        assert targets.shape == (batch_size, context_size)\n",
    "        assert (inputs[:, 1:] == targets[:,:-1]).all()\n",
    "        all_inputs.append(inputs.view(-1))\n",
    "        all_targets.append(targets.view(-1))\n",
    "\n",
    "    input_lst = list(sorted(torch.concat(all_inputs).tolist()))\n",
    "    target_lst = list(sorted(torch.concat(all_targets).tolist()))\n",
    "    assert len(input_lst) == len(data) - (len(data) % (context_size * batch_size))\n",
    "    assert input_lst == data[:len(input_lst)]\n",
    "\n",
    "    assert len(target_lst) == len(input_lst)\n",
    "    assert target_lst == data[1:(len(input_lst) + 1)]\n",
    "\n",
    "\n",
    "def _test_random_dataloader() -> None:\n",
    "    data = list(range(100))\n",
    "    context_size = 3\n",
    "    batch_size = 8\n",
    "    dataloader = RandomDataLoader(data, context_size, batch_size)\n",
    "    data = iter(dataloader)\n",
    "    inputs, targets = next(data)\n",
    "    assert inputs.shape == (batch_size, context_size)\n",
    "    assert targets.shape == (batch_size, context_size)\n",
    "    assert (inputs[:, 1:] == targets[:,:-1]).all()\n",
    "\n",
    "    inputs2, targets2 = next(data)\n",
    "    assert (inputs2 != inputs).any()\n",
    "    assert (targets2 != targets).any()\n",
    "\n",
    "\n",
    "_test_epoch_dataloader()\n",
    "_test_random_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcea231-2633-440e-9095-ec564cd3a8f4",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1e7c7c-6056-4168-951d-6612b47c7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:34:34,351 - INFO: Token count = 9 (' !dehlorw')\n"
     ]
    }
   ],
   "source": [
    "class CharacterEncoder:\n",
    "    def __init__(self, text: str):\n",
    "        tokens = sorted(list(set(text)))\n",
    "        self._token_lookup = {char: i for i, char in enumerate(tokens)}\n",
    "        self._reverse_token_lookup = {i: char for char, i in self._token_lookup.items()}\n",
    "\n",
    "        logger.info(f\"Token count = {len(tokens)} ({repr(''.join(tokens))})\")\n",
    "\n",
    "    @property\n",
    "    def n_vocab(self) -> int:\n",
    "        return len(self._token_lookup)\n",
    "\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return [self._token_lookup[char] for char in string]\n",
    "\n",
    "    def decode(self, values: str) -> str:\n",
    "        return \"\".join([self._reverse_token_lookup[value] for value in values])\n",
    "\n",
    "\n",
    "def get_encoding(name: str, *args, **kwargs):\n",
    "    if name == \"char\":\n",
    "        return CharacterEncoder(*args, **kwargs)\n",
    "    return tiktoken.get_encoding(name, *args, **kwargs)\n",
    "\n",
    "\n",
    "def _test_get_encoding() -> None:\n",
    "    text = \"hello world!\"\n",
    "    \n",
    "    encoder = get_encoding(\"gpt2\")\n",
    "    tokens = encoder.encode(text)\n",
    "    text_again = encoder.decode(tokens)\n",
    "    assert text_again == text\n",
    "    assert encoder.n_vocab == 50257\n",
    "\n",
    "    encoder = get_encoding(\"char\", text)\n",
    "    tokens = encoder.encode(text)\n",
    "    text_again = encoder.decode(tokens)\n",
    "    assert text_again == text\n",
    "    assert encoder.n_vocab == 9\n",
    "\n",
    "\n",
    "_test_get_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8e8ca-3702-4fea-93df-3a0d51d3244e",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091bb1c2-0b18-4073-a3d3-d778a695e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:34:34,355 - INFO: Loading https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt from cache at ./data/tiny_shakespeare.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data = 1,115,394\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "cache_path = \"./data/tiny_shakespeare.txt\"\n",
    "full_text = get_data(url, cache_path)\n",
    "print(f\"Length of training data = {len(full_text):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a756a633-25ba-4354-99bc-3fe0dad6267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model_settings = {\n",
    "    \"max_context_size\": 8,\n",
    "    \"n_embeddings\": 8 * 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"feed_forward_mult\": 4,\n",
    "    \"p_dropout\": 0.2,\n",
    "    \"ff_nonlinearity\": \"relu\",\n",
    "    \"num_blocks\": 3,\n",
    "}\n",
    "\n",
    "large_model_settings = {\n",
    "    \"max_context_size\": 128,\n",
    "    \"n_embeddings\": 32 * 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"feed_forward_mult\": 4,\n",
    "    \"p_dropout\": 0.2,\n",
    "    \"ff_nonlinearity\": \"relu\",\n",
    "    \"num_blocks\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24e5914-32eb-4ba1-aada-735bfb2fd81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:34:34,369 - INFO: Token count = 65 (\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")\n",
      "2025-01-23 14:34:34,369 - INFO: Encoder dimension 65\n"
     ]
    }
   ],
   "source": [
    "test_train_split = 0.9\n",
    "encoder = get_encoding(\"char\", full_text)\n",
    "logging.info(f\"Encoder dimension {encoder.n_vocab}\")\n",
    "\n",
    "settings = large_model_settings\n",
    "\n",
    "opt_settings = {\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"n_steps\": 10000,\n",
    "}\n",
    "\n",
    "reporting_settings = {\n",
    "    \"reports_per_epoch\": 2,\n",
    "    \"test_batches\": 50,\n",
    "}\n",
    "\n",
    "def cross_entropy_loss(inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    vocab_size = inputs.shape[-1]\n",
    "    return torch.nn.functional.cross_entropy(inputs.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "loss_fn = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db0371b-025e-4688-ba8c-d3dd659baaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:34:34,513 - INFO: Model parameter count = 824,897\n",
      "2025-01-23 14:34:34,513 - INFO: ---- _token_embedding_table.weight: 8,320\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _position_embedding_table.weight: 16,384\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _attention_blocks.0: 197,888\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _attention_blocks.1: 197,888\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _attention_blocks.2: 197,888\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _attention_blocks.3: 197,888\n",
      "2025-01-23 14:34:34,514 - INFO: ---- _final_layer_norm.weight: 128\n",
      "2025-01-23 14:34:34,515 - INFO: ---- _final_layer_norm.bias: 128\n",
      "2025-01-23 14:34:34,515 - INFO: ---- _final_linear.weight: 8,320\n",
      "2025-01-23 14:34:34,515 - INFO: ---- _final_linear.bias: 65\n",
      "2025-01-23 14:34:41,241 - INFO: Sample 1: \"\\n:HW!M&,CVtwGzO$V\\ns.NGfTX.C3h.B T.J'jJGpubMyllWidOV$yK \\nBcGrdYcR'G.us.wIDTX3IsU:'fCLsJGfF!FZZiVC;pMJg\"\n",
      "2025-01-23 14:34:41,940 - INFO: Initial test loss = 4.344±0.001\n",
      "2025-01-23 14:34:42,490 - INFO: Running 122 for every 82 epochs, total 10004 steps\n",
      "2025-01-23 14:34:42,490 - INFO: ----- Epoch 1 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:34:44,852 - INFO: Step 61: training_loss=2.867±0.003, test_loss=2.861±0.003, time_diff=2.36, token_rate=2.12e+05/s (remaining=385.0s)\n",
      "2025-01-23 14:34:46,983 - INFO: Step 122: training_loss=2.613±0.002, test_loss=2.606±0.002, time_diff=2.13, token_rate=2.35e+05/s (remaining=345.1s)\n",
      "2025-01-23 14:34:46,984 - INFO: ----- Epoch 2 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:34:49,072 - INFO: Step 183: training_loss=2.540±0.003, test_loss=2.530±0.002, time_diff=2.09, token_rate=2.39e+05/s (remaining=336.4s)\n",
      "2025-01-23 14:34:51,110 - INFO: Step 244: training_loss=2.500±0.002, test_loss=2.496±0.002, time_diff=2.04, token_rate=2.45e+05/s (remaining=326.2s)\n",
      "2025-01-23 14:34:51,111 - INFO: ----- Epoch 3 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:34:53,163 - INFO: Step 305: training_loss=2.472±0.003, test_loss=2.465±0.003, time_diff=2.05, token_rate=2.43e+05/s (remaining=326.4s)\n",
      "2025-01-23 14:34:55,178 - INFO: Step 366: training_loss=2.450±0.001, test_loss=2.442±0.002, time_diff=2.01, token_rate=2.48e+05/s (remaining=318.3s)\n",
      "2025-01-23 14:34:55,178 - INFO: ----- Epoch 4 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:34:57,256 - INFO: Step 427: training_loss=2.426±0.003, test_loss=2.417±0.002, time_diff=2.08, token_rate=2.4e+05/s (remaining=326.3s)\n",
      "2025-01-23 14:34:59,299 - INFO: Step 488: training_loss=2.401±0.001, test_loss=2.393±0.002, time_diff=2.04, token_rate=2.45e+05/s (remaining=318.7s)\n",
      "2025-01-23 14:34:59,300 - INFO: ----- Epoch 5 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:01,336 - INFO: Step 549: training_loss=2.373±0.004, test_loss=2.360±0.002, time_diff=2.04, token_rate=2.45e+05/s (remaining=315.8s)\n",
      "2025-01-23 14:35:03,342 - INFO: Step 610: training_loss=2.339±0.004, test_loss=2.330±0.002, time_diff=2.01, token_rate=2.49e+05/s (remaining=308.8s)\n",
      "2025-01-23 14:35:03,342 - INFO: ----- Epoch 6 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:05,395 - INFO: Step 671: training_loss=2.305±0.004, test_loss=2.285±0.002, time_diff=2.05, token_rate=2.43e+05/s (remaining=314.2s)\n",
      "2025-01-23 14:35:07,624 - INFO: Step 732: training_loss=2.265±0.003, test_loss=2.245±0.002, time_diff=2.23, token_rate=2.24e+05/s (remaining=338.8s)\n",
      "2025-01-23 14:35:07,625 - INFO: ----- Epoch 7 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:09,801 - INFO: Step 793: training_loss=2.232±0.002, test_loss=2.209±0.003, time_diff=2.18, token_rate=2.29e+05/s (remaining=328.8s)\n",
      "2025-01-23 14:35:11,873 - INFO: Step 854: training_loss=2.198±0.004, test_loss=2.178±0.003, time_diff=2.07, token_rate=2.41e+05/s (remaining=310.8s)\n",
      "2025-01-23 14:35:11,874 - INFO: ----- Epoch 8 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:13,881 - INFO: Step 915: training_loss=2.166±0.004, test_loss=2.159±0.003, time_diff=2.01, token_rate=2.49e+05/s (remaining=299.1s)\n",
      "2025-01-23 14:35:16,052 - INFO: Step 976: training_loss=2.141±0.005, test_loss=2.131±0.003, time_diff=2.17, token_rate=2.3e+05/s (remaining=321.3s)\n",
      "2025-01-23 14:35:16,054 - INFO: ----- Epoch 9 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:18,200 - INFO: Step 1037: training_loss=2.120±0.003, test_loss=2.110±0.003, time_diff=2.15, token_rate=2.33e+05/s (remaining=315.7s)\n",
      "2025-01-23 14:35:20,251 - INFO: Step 1098: training_loss=2.099±0.003, test_loss=2.093±0.003, time_diff=2.05, token_rate=2.44e+05/s (remaining=299.5s)\n",
      "2025-01-23 14:35:20,252 - INFO: ----- Epoch 10 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:22,408 - INFO: Step 1159: training_loss=2.074±0.003, test_loss=2.074±0.003, time_diff=2.16, token_rate=2.32e+05/s (remaining=312.8s)\n",
      "2025-01-23 14:35:24,469 - INFO: Step 1220: training_loss=2.056±0.005, test_loss=2.064±0.003, time_diff=2.06, token_rate=2.43e+05/s (remaining=296.7s)\n",
      "2025-01-23 14:35:24,469 - INFO: ----- Epoch 11 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:26,640 - INFO: Step 1281: training_loss=2.027±0.004, test_loss=2.045±0.002, time_diff=2.17, token_rate=2.3e+05/s (remaining=310.5s)\n",
      "2025-01-23 14:35:28,720 - INFO: Step 1342: training_loss=2.018±0.005, test_loss=2.034±0.003, time_diff=2.08, token_rate=2.4e+05/s (remaining=295.4s)\n",
      "2025-01-23 14:35:28,721 - INFO: ----- Epoch 12 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:30,719 - INFO: Step 1403: training_loss=1.995±0.002, test_loss=2.017±0.003, time_diff=2, token_rate=2.5e+05/s (remaining=281.9s)\n",
      "2025-01-23 14:35:32,726 - INFO: Step 1464: training_loss=1.982±0.007, test_loss=2.005±0.003, time_diff=2.01, token_rate=2.49e+05/s (remaining=281.0s)\n",
      "2025-01-23 14:35:32,727 - INFO: ----- Epoch 13 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:34,784 - INFO: Step 1525: training_loss=1.958±0.003, test_loss=1.988±0.003, time_diff=2.06, token_rate=2.43e+05/s (remaining=286.1s)\n",
      "2025-01-23 14:35:36,846 - INFO: Step 1586: training_loss=1.954±0.005, test_loss=1.975±0.003, time_diff=2.06, token_rate=2.42e+05/s (remaining=284.5s)\n",
      "2025-01-23 14:35:36,846 - INFO: ----- Epoch 14 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:38,893 - INFO: Step 1647: training_loss=1.925±0.006, test_loss=1.967±0.003, time_diff=2.05, token_rate=2.44e+05/s (remaining=280.5s)\n",
      "2025-01-23 14:35:40,863 - INFO: Step 1708: training_loss=1.920±0.007, test_loss=1.955±0.003, time_diff=1.97, token_rate=2.54e+05/s (remaining=267.9s)\n",
      "2025-01-23 14:35:40,864 - INFO: ----- Epoch 15 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:42,832 - INFO: Step 1769: training_loss=1.894±0.005, test_loss=1.949±0.003, time_diff=1.97, token_rate=2.54e+05/s (remaining=265.8s)\n",
      "2025-01-23 14:35:44,913 - INFO: Step 1830: training_loss=1.890±0.006, test_loss=1.935±0.003, time_diff=2.08, token_rate=2.4e+05/s (remaining=278.8s)\n",
      "2025-01-23 14:35:44,914 - INFO: ----- Epoch 16 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:46,986 - INFO: Step 1891: training_loss=1.867±0.005, test_loss=1.925±0.003, time_diff=2.07, token_rate=2.41e+05/s (remaining=275.8s)\n",
      "2025-01-23 14:35:49,015 - INFO: Step 1952: training_loss=1.863±0.007, test_loss=1.920±0.003, time_diff=2.03, token_rate=2.46e+05/s (remaining=267.8s)\n",
      "2025-01-23 14:35:49,016 - INFO: ----- Epoch 17 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:51,006 - INFO: Step 2013: training_loss=1.843±0.005, test_loss=1.912±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=260.9s)\n",
      "2025-01-23 14:35:53,000 - INFO: Step 2074: training_loss=1.843±0.007, test_loss=1.908±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=259.2s)\n",
      "2025-01-23 14:35:53,001 - INFO: ----- Epoch 18 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:55,208 - INFO: Step 2135: training_loss=1.819±0.006, test_loss=1.898±0.003, time_diff=2.21, token_rate=2.26e+05/s (remaining=284.8s)\n",
      "2025-01-23 14:35:57,301 - INFO: Step 2196: training_loss=1.820±0.008, test_loss=1.894±0.003, time_diff=2.09, token_rate=2.39e+05/s (remaining=267.8s)\n",
      "2025-01-23 14:35:57,302 - INFO: ----- Epoch 19 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:35:59,428 - INFO: Step 2257: training_loss=1.796±0.006, test_loss=1.888±0.003, time_diff=2.13, token_rate=2.35e+05/s (remaining=270.2s)\n",
      "2025-01-23 14:36:01,441 - INFO: Step 2318: training_loss=1.801±0.007, test_loss=1.881±0.003, time_diff=2.01, token_rate=2.48e+05/s (remaining=253.7s)\n",
      "2025-01-23 14:36:01,442 - INFO: ----- Epoch 20 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:03,498 - INFO: Step 2379: training_loss=1.783±0.008, test_loss=1.875±0.003, time_diff=2.06, token_rate=2.43e+05/s (remaining=257.1s)\n",
      "2025-01-23 14:36:05,582 - INFO: Step 2440: training_loss=1.784±0.008, test_loss=1.874±0.003, time_diff=2.08, token_rate=2.4e+05/s (remaining=258.4s)\n",
      "2025-01-23 14:36:05,584 - INFO: ----- Epoch 21 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:07,679 - INFO: Step 2501: training_loss=1.762±0.008, test_loss=1.863±0.003, time_diff=2.1, token_rate=2.38e+05/s (remaining=257.8s)\n",
      "2025-01-23 14:36:09,776 - INFO: Step 2562: training_loss=1.771±0.008, test_loss=1.858±0.004, time_diff=2.1, token_rate=2.38e+05/s (remaining=255.7s)\n",
      "2025-01-23 14:36:09,776 - INFO: ----- Epoch 22 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:11,876 - INFO: Step 2623: training_loss=1.749±0.006, test_loss=1.853±0.004, time_diff=2.1, token_rate=2.38e+05/s (remaining=254.1s)\n",
      "2025-01-23 14:36:13,919 - INFO: Step 2684: training_loss=1.749±0.010, test_loss=1.844±0.004, time_diff=2.04, token_rate=2.45e+05/s (remaining=245.1s)\n",
      "2025-01-23 14:36:13,919 - INFO: ----- Epoch 23 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:16,013 - INFO: Step 2745: training_loss=1.732±0.008, test_loss=1.849±0.003, time_diff=2.09, token_rate=2.39e+05/s (remaining=249.2s)\n",
      "2025-01-23 14:36:18,071 - INFO: Step 2806: training_loss=1.734±0.008, test_loss=1.839±0.003, time_diff=2.06, token_rate=2.43e+05/s (remaining=242.7s)\n",
      "2025-01-23 14:36:18,072 - INFO: ----- Epoch 24 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:20,074 - INFO: Step 2867: training_loss=1.719±0.006, test_loss=1.837±0.004, time_diff=2, token_rate=2.49e+05/s (remaining=234.4s)\n",
      "2025-01-23 14:36:22,074 - INFO: Step 2928: training_loss=1.724±0.006, test_loss=1.829±0.003, time_diff=2, token_rate=2.5e+05/s (remaining=232.0s)\n",
      "2025-01-23 14:36:22,075 - INFO: ----- Epoch 25 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:24,158 - INFO: Step 2989: training_loss=1.704±0.006, test_loss=1.823±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=239.6s)\n",
      "2025-01-23 14:36:26,270 - INFO: Step 3050: training_loss=1.712±0.012, test_loss=1.823±0.004, time_diff=2.11, token_rate=2.37e+05/s (remaining=240.7s)\n",
      "2025-01-23 14:36:26,270 - INFO: ----- Epoch 26 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:28,337 - INFO: Step 3111: training_loss=1.695±0.005, test_loss=1.818±0.003, time_diff=2.07, token_rate=2.42e+05/s (remaining=233.6s)\n",
      "2025-01-23 14:36:30,399 - INFO: Step 3172: training_loss=1.700±0.010, test_loss=1.817±0.003, time_diff=2.06, token_rate=2.42e+05/s (remaining=230.9s)\n",
      "2025-01-23 14:36:30,399 - INFO: ----- Epoch 27 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:32,459 - INFO: Step 3233: training_loss=1.680±0.006, test_loss=1.809±0.004, time_diff=2.06, token_rate=2.43e+05/s (remaining=228.7s)\n",
      "2025-01-23 14:36:34,540 - INFO: Step 3294: training_loss=1.689±0.011, test_loss=1.809±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=228.9s)\n",
      "2025-01-23 14:36:34,541 - INFO: ----- Epoch 28 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:36,599 - INFO: Step 3355: training_loss=1.671±0.007, test_loss=1.796±0.004, time_diff=2.06, token_rate=2.43e+05/s (remaining=224.4s)\n",
      "2025-01-23 14:36:38,601 - INFO: Step 3416: training_loss=1.682±0.010, test_loss=1.806±0.004, time_diff=2, token_rate=2.5e+05/s (remaining=216.2s)\n",
      "2025-01-23 14:36:38,602 - INFO: ----- Epoch 29 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:40,714 - INFO: Step 3477: training_loss=1.663±0.008, test_loss=1.790±0.004, time_diff=2.11, token_rate=2.36e+05/s (remaining=226.1s)\n",
      "2025-01-23 14:36:42,798 - INFO: Step 3538: training_loss=1.668±0.010, test_loss=1.795±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=220.9s)\n",
      "2025-01-23 14:36:42,799 - INFO: ----- Epoch 30 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:44,874 - INFO: Step 3599: training_loss=1.654±0.007, test_loss=1.788±0.004, time_diff=2.08, token_rate=2.41e+05/s (remaining=217.9s)\n",
      "2025-01-23 14:36:46,874 - INFO: Step 3660: training_loss=1.661±0.011, test_loss=1.796±0.004, time_diff=2, token_rate=2.5e+05/s (remaining=208.0s)\n",
      "2025-01-23 14:36:46,875 - INFO: ----- Epoch 31 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:48,840 - INFO: Step 3721: training_loss=1.644±0.006, test_loss=1.779±0.004, time_diff=1.97, token_rate=2.54e+05/s (remaining=202.5s)\n",
      "2025-01-23 14:36:50,902 - INFO: Step 3782: training_loss=1.654±0.012, test_loss=1.786±0.004, time_diff=2.06, token_rate=2.42e+05/s (remaining=210.3s)\n",
      "2025-01-23 14:36:50,903 - INFO: ----- Epoch 32 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:52,941 - INFO: Step 3843: training_loss=1.636±0.008, test_loss=1.773±0.004, time_diff=2.04, token_rate=2.45e+05/s (remaining=205.9s)\n",
      "2025-01-23 14:36:54,977 - INFO: Step 3904: training_loss=1.642±0.011, test_loss=1.768±0.003, time_diff=2.04, token_rate=2.45e+05/s (remaining=203.6s)\n",
      "2025-01-23 14:36:54,978 - INFO: ----- Epoch 33 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:36:56,954 - INFO: Step 3965: training_loss=1.630±0.006, test_loss=1.768±0.003, time_diff=1.98, token_rate=2.53e+05/s (remaining=195.7s)\n",
      "2025-01-23 14:36:58,982 - INFO: Step 4026: training_loss=1.641±0.011, test_loss=1.768±0.004, time_diff=2.03, token_rate=2.46e+05/s (remaining=198.8s)\n",
      "2025-01-23 14:36:58,983 - INFO: ----- Epoch 34 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:01,032 - INFO: Step 4087: training_loss=1.617±0.006, test_loss=1.761±0.004, time_diff=2.05, token_rate=2.44e+05/s (remaining=198.9s)\n",
      "2025-01-23 14:37:03,136 - INFO: Step 4148: training_loss=1.627±0.012, test_loss=1.768±0.004, time_diff=2.1, token_rate=2.38e+05/s (remaining=202.0s)\n",
      "2025-01-23 14:37:03,138 - INFO: ----- Epoch 35 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:05,214 - INFO: Step 4209: training_loss=1.613±0.005, test_loss=1.761±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=197.4s)\n",
      "2025-01-23 14:37:07,206 - INFO: Step 4270: training_loss=1.621±0.009, test_loss=1.761±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=187.2s)\n",
      "2025-01-23 14:37:07,206 - INFO: ----- Epoch 36 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:09,267 - INFO: Step 4331: training_loss=1.610±0.007, test_loss=1.752±0.004, time_diff=2.06, token_rate=2.42e+05/s (remaining=191.7s)\n",
      "2025-01-23 14:37:11,298 - INFO: Step 4392: training_loss=1.617±0.011, test_loss=1.757±0.004, time_diff=2.03, token_rate=2.46e+05/s (remaining=186.9s)\n",
      "2025-01-23 14:37:11,299 - INFO: ----- Epoch 37 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:13,391 - INFO: Step 4453: training_loss=1.599±0.006, test_loss=1.748±0.004, time_diff=2.09, token_rate=2.39e+05/s (remaining=190.4s)\n",
      "2025-01-23 14:37:15,509 - INFO: Step 4514: training_loss=1.614±0.010, test_loss=1.742±0.005, time_diff=2.12, token_rate=2.36e+05/s (remaining=190.7s)\n",
      "2025-01-23 14:37:15,510 - INFO: ----- Epoch 38 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:17,550 - INFO: Step 4575: training_loss=1.599±0.008, test_loss=1.741±0.004, time_diff=2.04, token_rate=2.45e+05/s (remaining=181.6s)\n",
      "2025-01-23 14:37:19,594 - INFO: Step 4636: training_loss=1.605±0.013, test_loss=1.748±0.004, time_diff=2.04, token_rate=2.45e+05/s (remaining=179.9s)\n",
      "2025-01-23 14:37:19,594 - INFO: ----- Epoch 39 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:21,679 - INFO: Step 4697: training_loss=1.590±0.006, test_loss=1.747±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=181.4s)\n",
      "2025-01-23 14:37:23,724 - INFO: Step 4758: training_loss=1.599±0.011, test_loss=1.747±0.004, time_diff=2.05, token_rate=2.44e+05/s (remaining=175.9s)\n",
      "2025-01-23 14:37:23,725 - INFO: ----- Epoch 40 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:25,736 - INFO: Step 4819: training_loss=1.584±0.006, test_loss=1.746±0.004, time_diff=2.01, token_rate=2.48e+05/s (remaining=171.0s)\n",
      "2025-01-23 14:37:27,855 - INFO: Step 4880: training_loss=1.589±0.011, test_loss=1.744±0.005, time_diff=2.12, token_rate=2.36e+05/s (remaining=178.0s)\n",
      "2025-01-23 14:37:27,855 - INFO: ----- Epoch 41 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:29,962 - INFO: Step 4941: training_loss=1.577±0.005, test_loss=1.737±0.004, time_diff=2.11, token_rate=2.37e+05/s (remaining=174.9s)\n",
      "2025-01-23 14:37:32,068 - INFO: Step 5002: training_loss=1.583±0.011, test_loss=1.742±0.004, time_diff=2.11, token_rate=2.37e+05/s (remaining=172.7s)\n",
      "2025-01-23 14:37:32,069 - INFO: ----- Epoch 42 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:34,159 - INFO: Step 5063: training_loss=1.573±0.005, test_loss=1.729±0.004, time_diff=2.09, token_rate=2.39e+05/s (remaining=169.4s)\n",
      "2025-01-23 14:37:36,183 - INFO: Step 5124: training_loss=1.578±0.013, test_loss=1.730±0.004, time_diff=2.02, token_rate=2.47e+05/s (remaining=161.9s)\n",
      "2025-01-23 14:37:36,183 - INFO: ----- Epoch 43 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:38,237 - INFO: Step 5185: training_loss=1.569±0.005, test_loss=1.737±0.004, time_diff=2.05, token_rate=2.43e+05/s (remaining=162.3s)\n",
      "2025-01-23 14:37:40,289 - INFO: Step 5246: training_loss=1.576±0.012, test_loss=1.730±0.004, time_diff=2.05, token_rate=2.44e+05/s (remaining=160.0s)\n",
      "2025-01-23 14:37:40,289 - INFO: ----- Epoch 44 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:42,506 - INFO: Step 5307: training_loss=1.561±0.006, test_loss=1.725±0.004, time_diff=2.22, token_rate=2.25e+05/s (remaining=170.7s)\n",
      "2025-01-23 14:37:44,677 - INFO: Step 5368: training_loss=1.570±0.012, test_loss=1.719±0.004, time_diff=2.17, token_rate=2.3e+05/s (remaining=165.0s)\n",
      "2025-01-23 14:37:44,678 - INFO: ----- Epoch 45 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:46,693 - INFO: Step 5429: training_loss=1.562±0.006, test_loss=1.726±0.004, time_diff=2.02, token_rate=2.48e+05/s (remaining=151.2s)\n",
      "2025-01-23 14:37:48,759 - INFO: Step 5490: training_loss=1.567±0.011, test_loss=1.719±0.004, time_diff=2.07, token_rate=2.42e+05/s (remaining=152.8s)\n",
      "2025-01-23 14:37:48,760 - INFO: ----- Epoch 46 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:50,954 - INFO: Step 5551: training_loss=1.552±0.004, test_loss=1.719±0.005, time_diff=2.2, token_rate=2.28e+05/s (remaining=160.3s)\n",
      "2025-01-23 14:37:53,046 - INFO: Step 5612: training_loss=1.562±0.012, test_loss=1.716±0.004, time_diff=2.09, token_rate=2.39e+05/s (remaining=150.6s)\n",
      "2025-01-23 14:37:53,046 - INFO: ----- Epoch 47 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:55,066 - INFO: Step 5673: training_loss=1.552±0.006, test_loss=1.720±0.005, time_diff=2.02, token_rate=2.47e+05/s (remaining=143.5s)\n",
      "2025-01-23 14:37:57,059 - INFO: Step 5734: training_loss=1.556±0.014, test_loss=1.712±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=139.5s)\n",
      "2025-01-23 14:37:57,059 - INFO: ----- Epoch 48 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:37:59,048 - INFO: Step 5795: training_loss=1.546±0.005, test_loss=1.705±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=137.3s)\n",
      "2025-01-23 14:38:01,073 - INFO: Step 5856: training_loss=1.552±0.012, test_loss=1.711±0.004, time_diff=2.03, token_rate=2.47e+05/s (remaining=137.7s)\n",
      "2025-01-23 14:38:01,074 - INFO: ----- Epoch 49 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:03,131 - INFO: Step 5917: training_loss=1.540±0.005, test_loss=1.717±0.004, time_diff=2.06, token_rate=2.43e+05/s (remaining=137.8s)\n",
      "2025-01-23 14:38:05,187 - INFO: Step 5978: training_loss=1.552±0.012, test_loss=1.703±0.004, time_diff=2.06, token_rate=2.43e+05/s (remaining=135.7s)\n",
      "2025-01-23 14:38:05,188 - INFO: ----- Epoch 50 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:07,215 - INFO: Step 6039: training_loss=1.534±0.005, test_loss=1.706±0.005, time_diff=2.03, token_rate=2.46e+05/s (remaining=131.8s)\n",
      "2025-01-23 14:38:09,283 - INFO: Step 6100: training_loss=1.545±0.012, test_loss=1.710±0.005, time_diff=2.07, token_rate=2.42e+05/s (remaining=132.3s)\n",
      "2025-01-23 14:38:09,284 - INFO: ----- Epoch 51 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:11,439 - INFO: Step 6161: training_loss=1.536±0.004, test_loss=1.705±0.005, time_diff=2.16, token_rate=2.32e+05/s (remaining=135.8s)\n",
      "2025-01-23 14:38:13,531 - INFO: Step 6222: training_loss=1.540±0.012, test_loss=1.700±0.005, time_diff=2.09, token_rate=2.39e+05/s (remaining=129.7s)\n",
      "2025-01-23 14:38:13,532 - INFO: ----- Epoch 52 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:15,574 - INFO: Step 6283: training_loss=1.535±0.003, test_loss=1.697±0.004, time_diff=2.04, token_rate=2.45e+05/s (remaining=124.6s)\n",
      "2025-01-23 14:38:17,632 - INFO: Step 6344: training_loss=1.537±0.013, test_loss=1.700±0.005, time_diff=2.06, token_rate=2.43e+05/s (remaining=123.5s)\n",
      "2025-01-23 14:38:17,633 - INFO: ----- Epoch 53 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:19,623 - INFO: Step 6405: training_loss=1.529±0.004, test_loss=1.691±0.004, time_diff=1.99, token_rate=2.51e+05/s (remaining=117.5s)\n",
      "2025-01-23 14:38:21,758 - INFO: Step 6466: training_loss=1.536±0.014, test_loss=1.704±0.004, time_diff=2.14, token_rate=2.34e+05/s (remaining=123.9s)\n",
      "2025-01-23 14:38:21,759 - INFO: ----- Epoch 54 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:23,809 - INFO: Step 6527: training_loss=1.524±0.005, test_loss=1.699±0.004, time_diff=2.05, token_rate=2.44e+05/s (remaining=116.9s)\n",
      "2025-01-23 14:38:25,843 - INFO: Step 6588: training_loss=1.532±0.013, test_loss=1.699±0.003, time_diff=2.03, token_rate=2.46e+05/s (remaining=113.9s)\n",
      "2025-01-23 14:38:25,843 - INFO: ----- Epoch 55 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:27,870 - INFO: Step 6649: training_loss=1.520±0.005, test_loss=1.698±0.005, time_diff=2.03, token_rate=2.47e+05/s (remaining=111.5s)\n",
      "2025-01-23 14:38:29,995 - INFO: Step 6710: training_loss=1.529±0.013, test_loss=1.692±0.004, time_diff=2.13, token_rate=2.35e+05/s (remaining=114.8s)\n",
      "2025-01-23 14:38:29,997 - INFO: ----- Epoch 56 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:32,055 - INFO: Step 6771: training_loss=1.515±0.004, test_loss=1.700±0.004, time_diff=2.06, token_rate=2.43e+05/s (remaining=109.2s)\n",
      "2025-01-23 14:38:34,110 - INFO: Step 6832: training_loss=1.525±0.012, test_loss=1.692±0.005, time_diff=2.05, token_rate=2.43e+05/s (remaining=106.9s)\n",
      "2025-01-23 14:38:34,111 - INFO: ----- Epoch 57 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:36,197 - INFO: Step 6893: training_loss=1.514±0.005, test_loss=1.692±0.005, time_diff=2.09, token_rate=2.39e+05/s (remaining=106.4s)\n",
      "2025-01-23 14:38:38,320 - INFO: Step 6954: training_loss=1.519±0.014, test_loss=1.704±0.004, time_diff=2.12, token_rate=2.35e+05/s (remaining=106.2s)\n",
      "2025-01-23 14:38:38,321 - INFO: ----- Epoch 58 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:40,457 - INFO: Step 7015: training_loss=1.512±0.005, test_loss=1.685±0.004, time_diff=2.14, token_rate=2.34e+05/s (remaining=104.7s)\n",
      "2025-01-23 14:38:42,546 - INFO: Step 7076: training_loss=1.519±0.013, test_loss=1.688±0.005, time_diff=2.09, token_rate=2.39e+05/s (remaining=100.3s)\n",
      "2025-01-23 14:38:42,547 - INFO: ----- Epoch 59 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:44,599 - INFO: Step 7137: training_loss=1.510±0.005, test_loss=1.682±0.005, time_diff=2.05, token_rate=2.43e+05/s (remaining=96.5s)\n",
      "2025-01-23 14:38:46,627 - INFO: Step 7198: training_loss=1.517±0.015, test_loss=1.681±0.005, time_diff=2.03, token_rate=2.46e+05/s (remaining=93.3s)\n",
      "2025-01-23 14:38:46,627 - INFO: ----- Epoch 60 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:48,651 - INFO: Step 7259: training_loss=1.505±0.008, test_loss=1.681±0.005, time_diff=2.02, token_rate=2.47e+05/s (remaining=91.1s)\n",
      "2025-01-23 14:38:50,702 - INFO: Step 7320: training_loss=1.512±0.011, test_loss=1.681±0.005, time_diff=2.05, token_rate=2.44e+05/s (remaining=90.3s)\n",
      "2025-01-23 14:38:50,703 - INFO: ----- Epoch 61 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:52,831 - INFO: Step 7381: training_loss=1.503±0.003, test_loss=1.681±0.004, time_diff=2.13, token_rate=2.35e+05/s (remaining=91.5s)\n",
      "2025-01-23 14:38:54,962 - INFO: Step 7442: training_loss=1.509±0.013, test_loss=1.678±0.004, time_diff=2.13, token_rate=2.35e+05/s (remaining=89.5s)\n",
      "2025-01-23 14:38:54,962 - INFO: ----- Epoch 62 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:38:57,079 - INFO: Step 7503: training_loss=1.498±0.007, test_loss=1.680±0.004, time_diff=2.12, token_rate=2.36e+05/s (remaining=86.8s)\n",
      "2025-01-23 14:38:59,258 - INFO: Step 7564: training_loss=1.506±0.013, test_loss=1.674±0.005, time_diff=2.18, token_rate=2.29e+05/s (remaining=87.2s)\n",
      "2025-01-23 14:38:59,260 - INFO: ----- Epoch 63 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:01,347 - INFO: Step 7625: training_loss=1.497±0.004, test_loss=1.683±0.005, time_diff=2.09, token_rate=2.39e+05/s (remaining=81.5s)\n",
      "2025-01-23 14:39:03,391 - INFO: Step 7686: training_loss=1.505±0.014, test_loss=1.678±0.005, time_diff=2.04, token_rate=2.44e+05/s (remaining=77.7s)\n",
      "2025-01-23 14:39:03,392 - INFO: ----- Epoch 64 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:05,426 - INFO: Step 7747: training_loss=1.499±0.006, test_loss=1.679±0.004, time_diff=2.03, token_rate=2.46e+05/s (remaining=75.3s)\n",
      "2025-01-23 14:39:07,485 - INFO: Step 7808: training_loss=1.501±0.012, test_loss=1.680±0.005, time_diff=2.06, token_rate=2.43e+05/s (remaining=74.1s)\n",
      "2025-01-23 14:39:07,486 - INFO: ----- Epoch 65 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:09,621 - INFO: Step 7869: training_loss=1.492±0.006, test_loss=1.676±0.005, time_diff=2.14, token_rate=2.34e+05/s (remaining=74.7s)\n",
      "2025-01-23 14:39:11,749 - INFO: Step 7930: training_loss=1.497±0.013, test_loss=1.675±0.004, time_diff=2.13, token_rate=2.35e+05/s (remaining=72.4s)\n",
      "2025-01-23 14:39:11,750 - INFO: ----- Epoch 66 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:13,818 - INFO: Step 7991: training_loss=1.487±0.006, test_loss=1.675±0.005, time_diff=2.07, token_rate=2.42e+05/s (remaining=68.3s)\n",
      "2025-01-23 14:39:15,894 - INFO: Step 8052: training_loss=1.494±0.013, test_loss=1.665±0.004, time_diff=2.08, token_rate=2.41e+05/s (remaining=66.4s)\n",
      "2025-01-23 14:39:15,894 - INFO: ----- Epoch 67 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:18,001 - INFO: Step 8113: training_loss=1.490±0.007, test_loss=1.681±0.005, time_diff=2.11, token_rate=2.37e+05/s (remaining=65.3s)\n",
      "2025-01-23 14:39:20,036 - INFO: Step 8174: training_loss=1.490±0.013, test_loss=1.678±0.005, time_diff=2.03, token_rate=2.46e+05/s (remaining=61.0s)\n",
      "2025-01-23 14:39:20,037 - INFO: ----- Epoch 68 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:22,117 - INFO: Step 8235: training_loss=1.486±0.004, test_loss=1.676±0.005, time_diff=2.08, token_rate=2.4e+05/s (remaining=60.4s)\n",
      "2025-01-23 14:39:24,214 - INFO: Step 8296: training_loss=1.495±0.013, test_loss=1.667±0.005, time_diff=2.1, token_rate=2.38e+05/s (remaining=58.7s)\n",
      "2025-01-23 14:39:24,215 - INFO: ----- Epoch 69 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:26,269 - INFO: Step 8357: training_loss=1.482±0.004, test_loss=1.658±0.005, time_diff=2.05, token_rate=2.43e+05/s (remaining=55.5s)\n",
      "2025-01-23 14:39:28,349 - INFO: Step 8418: training_loss=1.492±0.014, test_loss=1.668±0.005, time_diff=2.08, token_rate=2.4e+05/s (remaining=54.1s)\n",
      "2025-01-23 14:39:28,350 - INFO: ----- Epoch 70 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:30,434 - INFO: Step 8479: training_loss=1.485±0.009, test_loss=1.667±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=52.1s)\n",
      "2025-01-23 14:39:32,483 - INFO: Step 8540: training_loss=1.482±0.014, test_loss=1.666±0.004, time_diff=2.05, token_rate=2.44e+05/s (remaining=49.2s)\n",
      "2025-01-23 14:39:32,483 - INFO: ----- Epoch 71 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:34,550 - INFO: Step 8601: training_loss=1.478±0.005, test_loss=1.667±0.005, time_diff=2.07, token_rate=2.42e+05/s (remaining=47.5s)\n",
      "2025-01-23 14:39:36,676 - INFO: Step 8662: training_loss=1.484±0.013, test_loss=1.658±0.004, time_diff=2.13, token_rate=2.35e+05/s (remaining=46.8s)\n",
      "2025-01-23 14:39:36,678 - INFO: ----- Epoch 72 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:38,806 - INFO: Step 8723: training_loss=1.473±0.006, test_loss=1.674±0.005, time_diff=2.13, token_rate=2.35e+05/s (remaining=44.7s)\n",
      "2025-01-23 14:39:41,059 - INFO: Step 8784: training_loss=1.482±0.014, test_loss=1.663±0.004, time_diff=2.25, token_rate=2.22e+05/s (remaining=45.1s)\n",
      "2025-01-23 14:39:41,061 - INFO: ----- Epoch 73 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:43,240 - INFO: Step 8845: training_loss=1.474±0.004, test_loss=1.671±0.004, time_diff=2.18, token_rate=2.29e+05/s (remaining=41.4s)\n",
      "2025-01-23 14:39:45,320 - INFO: Step 8906: training_loss=1.476±0.014, test_loss=1.667±0.004, time_diff=2.08, token_rate=2.4e+05/s (remaining=37.4s)\n",
      "2025-01-23 14:39:45,321 - INFO: ----- Epoch 74 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:47,409 - INFO: Step 8967: training_loss=1.471±0.004, test_loss=1.653±0.004, time_diff=2.09, token_rate=2.39e+05/s (remaining=35.5s)\n",
      "2025-01-23 14:39:49,525 - INFO: Step 9028: training_loss=1.480±0.015, test_loss=1.663±0.005, time_diff=2.12, token_rate=2.36e+05/s (remaining=33.9s)\n",
      "2025-01-23 14:39:49,526 - INFO: ----- Epoch 75 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:51,932 - INFO: Step 9089: training_loss=1.473±0.005, test_loss=1.661±0.005, time_diff=2.41, token_rate=2.08e+05/s (remaining=36.1s)\n",
      "2025-01-23 14:39:54,217 - INFO: Step 9150: training_loss=1.476±0.014, test_loss=1.672±0.005, time_diff=2.29, token_rate=2.19e+05/s (remaining=32.0s)\n",
      "2025-01-23 14:39:54,218 - INFO: ----- Epoch 76 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:39:56,533 - INFO: Step 9211: training_loss=1.466±0.005, test_loss=1.665±0.005, time_diff=2.32, token_rate=2.16e+05/s (remaining=30.1s)\n",
      "2025-01-23 14:39:59,504 - INFO: Step 9272: training_loss=1.472±0.015, test_loss=1.661±0.005, time_diff=2.97, token_rate=1.68e+05/s (remaining=35.6s)\n",
      "2025-01-23 14:39:59,508 - INFO: ----- Epoch 77 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:02,052 - INFO: Step 9333: training_loss=1.466±0.007, test_loss=1.660±0.005, time_diff=2.55, token_rate=1.96e+05/s (remaining=28.0s)\n",
      "2025-01-23 14:40:04,143 - INFO: Step 9394: training_loss=1.470±0.012, test_loss=1.661±0.004, time_diff=2.09, token_rate=2.39e+05/s (remaining=20.9s)\n",
      "2025-01-23 14:40:04,144 - INFO: ----- Epoch 78 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:06,280 - INFO: Step 9455: training_loss=1.464±0.006, test_loss=1.651±0.005, time_diff=2.14, token_rate=2.34e+05/s (remaining=19.2s)\n",
      "2025-01-23 14:40:08,449 - INFO: Step 9516: training_loss=1.470±0.014, test_loss=1.666±0.005, time_diff=2.17, token_rate=2.3e+05/s (remaining=17.4s)\n",
      "2025-01-23 14:40:08,450 - INFO: ----- Epoch 79 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:10,637 - INFO: Step 9577: training_loss=1.467±0.004, test_loss=1.662±0.004, time_diff=2.19, token_rate=2.28e+05/s (remaining=15.3s)\n",
      "2025-01-23 14:40:12,768 - INFO: Step 9638: training_loss=1.465±0.016, test_loss=1.658±0.005, time_diff=2.13, token_rate=2.35e+05/s (remaining=12.8s)\n",
      "2025-01-23 14:40:12,769 - INFO: ----- Epoch 80 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:14,822 - INFO: Step 9699: training_loss=1.462±0.008, test_loss=1.655±0.005, time_diff=2.05, token_rate=2.43e+05/s (remaining=10.3s)\n",
      "2025-01-23 14:40:16,836 - INFO: Step 9760: training_loss=1.466±0.013, test_loss=1.658±0.005, time_diff=2.01, token_rate=2.48e+05/s (remaining=8.1s)\n",
      "2025-01-23 14:40:16,836 - INFO: ----- Epoch 81 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:18,863 - INFO: Step 9821: training_loss=1.461±0.004, test_loss=1.657±0.005, time_diff=2.03, token_rate=2.46e+05/s (remaining=6.1s)\n",
      "2025-01-23 14:40:20,954 - INFO: Step 9882: training_loss=1.465±0.013, test_loss=1.655±0.006, time_diff=2.09, token_rate=2.39e+05/s (remaining=4.2s)\n",
      "2025-01-23 14:40:20,955 - INFO: ----- Epoch 82 --- (lr = 0.0003) -----\n",
      "2025-01-23 14:40:22,982 - INFO: Step 9943: training_loss=1.458±0.006, test_loss=1.654±0.005, time_diff=2.03, token_rate=2.46e+05/s (remaining=2.0s)\n",
      "2025-01-23 14:40:25,006 - INFO: Step 10004: training_loss=1.461±0.016, test_loss=1.645±0.004, time_diff=2.02, token_rate=2.47e+05/s (remaining=0.0s)\n",
      "2025-01-23 14:40:25,429 - INFO: Final: training_loss=1.446, test_loss=1.654±0.005\n",
      "2025-01-23 14:40:35,109 - INFO: Sample 1: \"\\nVERCUTIO:\\nWhat thou must is bread fair about,\\nMen'd usuirit was detemped he eyes to and mothe\\nAnd ke\"\n",
      "2025-01-23 14:40:35,110 - INFO: Sample 2: '\\nThy supers in very twod.\\nFirst, agon tell me make those it law,\\nFight, if Margareretsment! What ever'\n",
      "2025-01-23 14:40:35,111 - INFO: Sample 3: \"\\nI'll full. That stay you must ours of your gave:\\n'Tis briggrace, Jather soul.\\n\\nSecome,\\nAnd Claudioli\"\n"
     ]
    }
   ],
   "source": [
    "n_split = int(test_train_split * len(full_text))\n",
    "train_text = full_text[:n_split]\n",
    "test_text = full_text[n_split:]\n",
    "\n",
    "set_torch_seed(123)\n",
    "\n",
    "training_dataloader = EpochDataLoader(encoder.encode(train_text), settings[\"max_context_size\"], opt_settings[\"batch_size\"])\n",
    "testing_dataloader = RandomDataLoader(encoder.encode(test_text), settings[\"max_context_size\"], opt_settings[\"batch_size\"])\n",
    "\n",
    "model = Model(\n",
    "    vocab_size=encoder.n_vocab,\n",
    "    n_embeddings=settings[\"n_embeddings\"],\n",
    "    max_context_size=settings[\"max_context_size\"],\n",
    "    num_attention_heads=settings[\"num_attention_heads\"],\n",
    "    feed_forward_mult=settings[\"feed_forward_mult\"],\n",
    "    p_dropout=settings[\"p_dropout\"],\n",
    "    ff_nonlinearity=settings[\"ff_nonlinearity\"],\n",
    "    num_blocks=settings[\"num_blocks\"],\n",
    ")\n",
    "torch_device = get_torch_device()\n",
    "model = model.to(torch_device)\n",
    "logging.info(f\"Model parameter count = {count_params(model, 0)['']:,}\")\n",
    "for desc, count in count_params(model, 2).items():\n",
    "    logging.info(f\"---- {desc}: {count:,}\")\n",
    "samples = generate_samples(model, \"\\n\", encoder, 100, 1)\n",
    "for i, sample in enumerate(samples):\n",
    "    logging.info(f\"Sample {i + 1}: {repr(sample)}\")\n",
    "test_loss, test_loss_std = estimate_loss(model, loss_fn, testing_dataloader, reporting_settings[\"test_batches\"])\n",
    "logging.info(f\"Initial test loss = {test_loss:.3f}±{test_loss_std:.3f}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opt_settings[\"learning_rate\"])\n",
    "\n",
    "steps_per_epoch = len(training_dataloader)\n",
    "n_epochs = math.ceil(opt_settings[\"n_steps\"] / steps_per_epoch)\n",
    "logging.info(f\"Running {steps_per_epoch} for every {n_epochs} epochs, total {n_epochs * steps_per_epoch} steps\")\n",
    "report_interval = steps_per_epoch // reporting_settings[\"reports_per_epoch\"]\n",
    "n_batches = []\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "times = []\n",
    "training_losses_accumulator = []\n",
    "\n",
    "start_time = time.monotonic()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    logger.info(f\"----- Epoch {epoch + 1} --- (lr = {optimizer.param_groups[0][\"lr\"]:.3g}) -----\")\n",
    "    for step, (inputs, targets) in enumerate(training_dataloader):\n",
    "        full_step = epoch * steps_per_epoch + step\n",
    "        logits = model(inputs.to(torch_device))\n",
    "        loss = loss_fn(logits, targets.to(torch_device))\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ((step + 1) % report_interval) > (report_interval - 5):\n",
    "            training_losses_accumulator.append(loss.item())\n",
    "    \n",
    "        if (step + 1) % report_interval == 0:\n",
    "            training_loss = np.mean(training_losses_accumulator)\n",
    "            training_loss_std = np.std(training_losses_accumulator) / np.sqrt(len(training_losses_accumulator))\n",
    "            training_losses_accumulator = []\n",
    "            test_loss, test_loss_std = estimate_loss(model, loss_fn, testing_dataloader, reporting_settings[\"test_batches\"])\n",
    "            \n",
    "            now_time = time.monotonic()\n",
    "            time_diff = now_time - start_time\n",
    "            if step != 0:\n",
    "                token_rate = inputs.numel() * report_interval / time_diff\n",
    "            else:\n",
    "                token_rate = inputs.numel() / time_diff\n",
    "            start_time = now_time\n",
    "    \n",
    "            n_batches.append(full_step)\n",
    "            training_losses.append(training_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            times.append(time_diff)\n",
    "            remaining_time = (steps_per_epoch - step - 1 + (n_epochs - epoch - 1) * steps_per_epoch) * time_diff / report_interval\n",
    "            msg = (\n",
    "                f\"Step {full_step + 1}: training_loss={training_loss:.3f}±{training_loss_std:.3f}, \"\n",
    "                f\"test_loss={test_loss:.3f}±{test_loss_std:.3f}, \"\n",
    "                f\"time_diff={time_diff:.3g}, \"\n",
    "                f\"token_rate={token_rate:.3g}/s \"\n",
    "                f\"(remaining={remaining_time:.1f}s)\"\n",
    "            )\n",
    "            logger.info(msg)\n",
    "\n",
    "test_loss, test_loss_std = estimate_loss(model, loss_fn, testing_dataloader, reporting_settings[\"test_batches\"])\n",
    "logger.info(f\"Final: training_loss={loss.item():.3f}, test_loss={test_loss:.3f}±{test_loss_std:.3f}\")\n",
    "samples = generate_samples(model, \"\\n\", encoder, 100, 3)\n",
    "for i, sample in enumerate(samples):\n",
    "    logging.info(f\"Sample {i + 1}: {repr(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbc99be-f1c6-4194-bd02-1c4ad7b24372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(training_losses: list[float], test_losses: list[float], n_batches: list[int]) -> None:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(n_batches, training_losses, label='Training Loss')\n",
    "    plt.plot(n_batches, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Test Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(training_losses, test_losses, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd046ec8-c341-4189-8d12-6d4c2a6cb193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyproject Local",
   "language": "python",
   "name": "pyproject_local_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
