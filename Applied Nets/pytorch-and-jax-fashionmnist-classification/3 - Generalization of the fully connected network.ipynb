{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0d6b03-5951-4421-b952-935b89cd82ce",
   "metadata": {},
   "source": [
    "# Exploring whether the feed-forward fully-connected network generalizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e83510-b875-4b12-bc6d-32e796dfdd83",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff63840-0999-4e79-8376-81d2b0d2647b",
   "metadata": {},
   "source": [
    "- This notebook explores whether the fully-connected feed-forward network trained on FashionMNIST can generalize to additional transformations not in the input data\n",
    "- It can't. While in theory a fully-connected network can learn symmetries (eg translation, scale), it needs to see these in the input data. The raw FashionMNIST data does not have these symmetries.\n",
    "- By manipulating the training data, it is possible to add the symmetries.\n",
    "- However, it could be more efficient to embed these symmetries directly in the network, by reducing the connections, and by using parameter sharing (ie the same weights for multiple units). This is left to a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69813093-0643-48bd-afcb-eed40614fb57",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6167f55-d34f-4f84-bf37-0c429bf606cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "TORCH_DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d006a32c-2627-4a04-953c-37c9b7b8d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size: int, train: bool, transform=None):\n",
    "    if transform is None:\n",
    "        transform = v2.ToTensor()\n",
    "    else:\n",
    "        transform = v2.Compose([transform, v2.ToTensor()])\n",
    "    mnist_dataset = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return data.DataLoader(mnist_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c757a0-007a-465b-bd09-5219baad9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LossRecord:\n",
    "    name: str\n",
    "    accuracy: list[float] = field(default_factory=list)\n",
    "    loss: list[float] = field(default_factory=list)\n",
    "\n",
    "    def append(self, accuracy, loss) -> None:\n",
    "        self.accuracy.append(accuracy)\n",
    "        self.loss.append(loss)\n",
    "\n",
    "\n",
    "class Optim:\n",
    "    \"\"\"A wrapper to run the optimization with some logging.\"\"\"\n",
    "    def __init__(self, model, loss_fn, training_data, testing_data):\n",
    "        self._model = model\n",
    "        self._loss_fn = loss_fn\n",
    "        self._training_data = training_data\n",
    "        self._testing_data = testing_data\n",
    "\n",
    "        self.log_training: int | None = None\n",
    "        self.log_epochs: int | None = 1\n",
    "\n",
    "    def _train_one_epoch(self, optimizer):\n",
    "        self._model.train()\n",
    "        num_batches = len(self._training_data)\n",
    "        # num_rows = len(dataloader.dataset)\n",
    "        log_every = None if self.log_training is None else num_batches / self.log_training\n",
    "        for i_batch, (inputs, targets) in enumerate(self._training_data):\n",
    "            inputs = inputs.to(TORCH_DEVICE)\n",
    "            targets = targets.to(TORCH_DEVICE)\n",
    "            predictions = self._model(inputs)\n",
    "            batch_loss = self._loss_fn(predictions, targets)\n",
    "\n",
    "            if log_every is not None and (log_every == num_batches or i_batch % log_every < 1 or i_batch == num_batches - 1):\n",
    "                msg = f\"Batch {i_batch} / {num_batches}: loss={batch_loss.item():>7f}\"\n",
    "                logger.info(msg)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    def test(self, testing_data_override = None):\n",
    "        testing_data = testing_data_override if testing_data_override is not None else self._testing_data\n",
    "        \n",
    "        self._model.eval()\n",
    "        total_loss = 0.\n",
    "        total_correct = 0.\n",
    "        n_rows = 0\n",
    "        n_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in testing_data:\n",
    "                inputs = inputs.to(TORCH_DEVICE)\n",
    "                targets = targets.to(TORCH_DEVICE)\n",
    "                predictions = model(inputs)\n",
    "                total_correct += (predictions.argmax(dim=1) == targets).sum().item()\n",
    "                total_loss += self._loss_fn(predictions, targets).item()\n",
    "                n_rows += len(targets)\n",
    "                n_batches += 1\n",
    "\n",
    "        accuracy = total_correct / n_rows\n",
    "        average_loss = total_loss / n_batches\n",
    "\n",
    "        return accuracy, average_loss\n",
    "                \n",
    "\n",
    "    def run(self, num_epochs, learning_rate, name) -> LossRecord:\n",
    "        optimizer = torch.optim.SGD(self._model.parameters(), lr=learning_rate)\n",
    "        record = LossRecord(name)\n",
    "        accuracy, loss = self.test()\n",
    "        record.append(accuracy, loss)\n",
    "        start_time = time.monotonic()\n",
    "        log_every = None if self.log_epochs is None else num_epochs / self.log_epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_start = time.monotonic()\n",
    "            self._train_one_epoch(optimizer)\n",
    "            epoch_end = time.monotonic()\n",
    "            accuracy, loss = self.test()\n",
    "            record.append(accuracy, loss)\n",
    "            if log_every is not None and (log_every == num_epochs or epoch % log_every < 1 or epoch == num_epochs - 1):\n",
    "                msg = f\"Epoch {epoch + 1}: accuracy {100 * accuracy:>0.1f}%, loss {loss:>8f} \"\n",
    "                msg += f\"(time {epoch_end - epoch_start:.1f}s)\"\n",
    "                logger.info(msg)\n",
    "        end_time = time.monotonic()\n",
    "        logger.info(f\"Finished {num_epochs} epochs for {name} in {end_time - start_time:.1f}s\")\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c5fb22-7de4-4298-92e7-3d0586751ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(dim_in, dim_hidden, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_hidden, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_out, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c3c32-c76e-42c1-91cc-2843b8be5cfa",
   "metadata": {},
   "source": [
    "## The regular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2289fa41-dcf9-4e64-a795-0638e99d6b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/Code/Projects/github/public/neural-network-notebooks/Applied Nets/pytorch-and-jax-fashionmnist-classification/.venv/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "2025-01-20 16:04:44,272 - INFO: Epoch 1: accuracy 77.1%, loss 0.632176 (time 3.2s)\n",
      "2025-01-20 16:04:47,337 - INFO: Epoch 2: accuracy 80.3%, loss 0.533694 (time 2.6s)\n",
      "2025-01-20 16:04:50,211 - INFO: Epoch 3: accuracy 81.8%, loss 0.493119 (time 2.4s)\n",
      "2025-01-20 16:04:53,239 - INFO: Epoch 4: accuracy 82.9%, loss 0.460780 (time 2.5s)\n",
      "2025-01-20 16:04:56,232 - INFO: Epoch 5: accuracy 83.8%, loss 0.437845 (time 2.6s)\n",
      "2025-01-20 16:04:59,121 - INFO: Epoch 6: accuracy 84.3%, loss 0.423843 (time 2.5s)\n",
      "2025-01-20 16:05:02,116 - INFO: Epoch 7: accuracy 84.7%, loss 0.410176 (time 2.5s)\n",
      "2025-01-20 16:05:05,082 - INFO: Epoch 8: accuracy 85.0%, loss 0.406648 (time 2.5s)\n",
      "2025-01-20 16:05:08,037 - INFO: Epoch 9: accuracy 85.7%, loss 0.390680 (time 2.5s)\n",
      "2025-01-20 16:05:11,100 - INFO: Epoch 10: accuracy 86.4%, loss 0.377938 (time 2.6s)\n",
      "2025-01-20 16:05:11,101 - INFO: Finished 10 epochs for Test in 30.5s\n"
     ]
    }
   ],
   "source": [
    "dim_hidden = 512\n",
    "dim_out = 10\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.05\n",
    "\n",
    "training_data = get_dataloader(batch_size=64, train=True)\n",
    "testing_data = get_dataloader(batch_size=64, train=False)\n",
    "for inputs, targets in training_data:\n",
    "    _, pic_channels, pic_height, pic_width = inputs.shape\n",
    "    break\n",
    "\n",
    "set_seed(12345678)\n",
    "model = SimpleNetwork(pic_channels * pic_height * pic_width, dim_hidden, dim_out).to(TORCH_DEVICE)\n",
    "optim = Optim(model, loss_fn, training_data, testing_data)\n",
    "optim.log_epochs = 1\n",
    "record = optim.run(num_epochs=10, learning_rate=learning_rate, name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c177317b-81a7-44bf-b625-a006163ffd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altering testing data changes accuracy 0.8637 -> 0.367\n",
      "Altering testing data changes loss 0.37793807391148465 -> 2.1588106793203172\n"
     ]
    }
   ],
   "source": [
    "zoom_out = v2.Compose([\n",
    "    v2.RandomZoomOut(fill=0, side_range=(1.25, 1.5), p=1),\n",
    "    v2.Resize((pic_height, pic_width), )\n",
    "])\n",
    "altered_testing_data = get_dataloader(batch_size=64, train=False, transform=zoom_out)\n",
    "\n",
    "altered = optim.test(altered_testing_data)\n",
    "regular = optim.test()\n",
    "\n",
    "print(f\"Altering testing data changes accuracy {regular[0]} -> {altered[0]}\")\n",
    "print(f\"Altering testing data changes loss {regular[1]} -> {altered[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3546a4-81f4-403c-af42-fbd4b52a0aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for inputs, targets in testing_data:\n",
    "    \n",
    "    axs[0].imshow(inputs[0, 0], cmap=\"gray\")\n",
    "\n",
    "    img = zoom_out(inputs[0])\n",
    "    axs[1].imshow(img[0], cmap=\"gray\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37d82e-7ac4-4d81-9311-827ea15ba79f",
   "metadata": {},
   "source": [
    "## Training with transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e02e08-7f69-414a-8baa-f12327ea4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_zoom_out = v2.Compose([\n",
    "    v2.RandomZoomOut(fill=0, side_range=(1., 1.8), p=1),\n",
    "    v2.Resize((pic_height, pic_width), )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d42a5f-2c98-41e5-a3fd-b95d0e36b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 16:05:18,573 - INFO: Epoch 1: accuracy 66.2%, loss 0.849006 (time 5.1s)\n",
      "2025-01-20 16:05:24,280 - INFO: Epoch 2: accuracy 72.9%, loss 0.709323 (time 5.3s)\n",
      "2025-01-20 16:05:29,767 - INFO: Epoch 3: accuracy 76.2%, loss 0.636296 (time 5.1s)\n",
      "2025-01-20 16:05:35,217 - INFO: Epoch 4: accuracy 77.6%, loss 0.591320 (time 5.0s)\n",
      "2025-01-20 16:05:40,955 - INFO: Epoch 5: accuracy 79.2%, loss 0.554916 (time 5.3s)\n",
      "2025-01-20 16:05:46,460 - INFO: Epoch 6: accuracy 79.1%, loss 0.566624 (time 5.1s)\n",
      "2025-01-20 16:05:51,918 - INFO: Epoch 7: accuracy 80.7%, loss 0.530551 (time 5.0s)\n",
      "2025-01-20 16:05:57,414 - INFO: Epoch 8: accuracy 81.7%, loss 0.512308 (time 5.1s)\n",
      "2025-01-20 16:06:02,855 - INFO: Epoch 9: accuracy 82.2%, loss 0.500578 (time 5.0s)\n",
      "2025-01-20 16:06:08,386 - INFO: Epoch 10: accuracy 82.3%, loss 0.492520 (time 5.1s)\n",
      "2025-01-20 16:06:08,387 - INFO: Finished 10 epochs for Test in 55.4s\n"
     ]
    }
   ],
   "source": [
    "dim_hidden = 512\n",
    "dim_out = 10\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 0.05\n",
    "\n",
    "training_data = get_dataloader(batch_size=64, train=True, transform=training_zoom_out)\n",
    "testing_data = get_dataloader(batch_size=64, train=False)\n",
    "for inputs, targets in training_data:\n",
    "    _, pic_channels, pic_height, pic_width = inputs.shape\n",
    "    break\n",
    "\n",
    "set_seed(12345678)\n",
    "model = SimpleNetwork(pic_channels * pic_height * pic_width, dim_hidden, dim_out).to(TORCH_DEVICE)\n",
    "optim = Optim(model, loss_fn, training_data, testing_data)\n",
    "optim.log_epochs = 1\n",
    "record = optim.run(num_epochs=10, learning_rate=learning_rate, name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfda3f7-8947-464e-8148-e17d43f00f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altering testing data changes accuracy 0.8226 -> 0.7582\n",
      "Altering testing data changes loss 0.4925200268151654 -> 0.6298230478338375\n"
     ]
    }
   ],
   "source": [
    "altered_testing_data = get_dataloader(batch_size=64, train=False, transform=zoom_out)\n",
    "altered = optim.test(altered_testing_data)\n",
    "regular = optim.test()\n",
    "\n",
    "print(f\"Altering testing data changes accuracy {regular[0]} -> {altered[0]}\")\n",
    "print(f\"Altering testing data changes loss {regular[1]} -> {altered[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5b96b-1ff7-4efc-89da-89aa9ffd1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyproject Local",
   "language": "python",
   "name": "pyproject_local_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
